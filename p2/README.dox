/**

@mainpage 15-410 Project 2

@author Nicklaus Choo (nchoo)
@author Andre Nascimento (anascime)

## Illegal operations

Our approach to illegal operations is to do everything in our immediate means to
deal with them. Some illegal operations, however, are quite hard to deal with
such as locking a mutex which is being destroyed. To the extent possible we
panic and crash the application in these cases, but at times, it simply leads
to undocumented behavior. We believe this is reasonable as the resources
required to better deal with these errors are not worth pursuing:
it's not reasonable to lockdown the entire park and begin an investigation
because someone littered. It's also not reasonable to have a guard closely
monitoring each visitant to stop them and deal with their littering.

## Error handling

We performed case analysis on how to deal with each bug. We have found, though,
that in the vast majority of cases, we return -1 on functions that are allowed
to fail and we crash on those that can't if we face an unfixable error. Also,
whenever we encounter a bug we make sure to cleanup all resources in use.
A notable case is locks: In case lock fails, it always crashes as a silent
return would give a thread the wrong notion that it has exclusive access to
the critical section.

When a given thread crashes, we call task void and print out to the user both
the panic message and the ID of the thread calling panic. The kernel is then
responsible for cleaning up.

## With respect to dynamic memory allocation

Whenever creating structs which have to be malloc'ed, we avoid making its
fields pointers if possible. By having the fields be values we can make fewer
calls to malloc and free, and we're less likely to forget to clean someone up.

## On synchronization concerns

We avoid using global locks as much as possible. With respect to the thread
library, we use a lock to store and retrieve shared status information. For
the malloc library we also use a global lock to avoid making any assumptions
about the provided malloc code. Using a global lock allows us to serialize
requests and have the same behavior as that of a single thread using malloc.

----- BELOW WE LIST ALL COMPONENTS OF THE CODE AND THE DESIGN BEHIND THEM -----

## Hashmap

The hashmap is implemented as an array of linked lists. Its size
is fixed to 1024, a reasonable number to avoid collisions. As
they lead to longer lookup times (linked list traversal).  We
use a hash function defined in github.com/skeeto/hash-prospector
for the same purpose.
The hashmap is used to store thread status information in a
globally accessible location. It is a global variable.

## Mutex

The mutex is implemented via a ticketing system. On lock
a thread atomically gets a ticket (add_one_atomic) and
waits until it is their turn. To solve the problem of
communication among threads (ie. thread 7 should let
thread 8 know it is done) we use a shared memory location,
namely the now_serving variable.

Threads repeatedly check it, yielding to the current lock
owner to avoid wasting cycles, and eventually acquire the lock.

Unlocking simply consists of update the now_serving variable
with a blind write.

## Read/Write locks

A read/write lock must allow either one writer or multiple readers
at a time. The approach here is to try and serve readers or writers
roughly in the order of arrival. Suppose the following order of
requests:

W1 R1 R2 W2

Then, we could avoid starvation of readers and writers by letting
W1 go first, then once its done we let R1 R2 go simulateneously,
and lastly, we let W2 go. However, this simple policy is can be rather
bad. Suppose the following sequence:

W1 R1 R2 W2 R3 R4

Then, if we had known of requests R3 and R4 before W1 gave
up its lock, we could reorganize our serving order as:

W1 R1 R2 R3 R4 W2

This enables us to serve all 4 readers in parallel. To enable
this "coalescing" of readers we make the following policy:

- Write requests go into a writer queue
- Read requests go into a reader queue
- Whenever we transition from WRITING to READING we service *all*
  read requests.

This simple policy equates to "coalescing" all reads together.
Wouldn't this lead to a starvation of writers, though? No. To avoid
the starvation of writers, we only coalesce when a writer has the lock.
When we are in the reading phase we accept no new readers.

Formulated more simply, we only "coalesce" readers during phase
transitions, which allows us to avoid starvation while enabling
high read parallelism.



*/

